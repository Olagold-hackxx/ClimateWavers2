{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc0ef2f-1813-4725-9672-ed3090e83ae2",
   "metadata": {},
   "source": [
    "# Climate wavers WaverX-NLP\n",
    "WaverX-NLP is climate wavers natural language processing model created by finetuning Bert with the huggingface transformer library.\n",
    "Let start by cloning waverX-NLP microservice repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aec898-7bcc-4a01-92d2-5ec4fd45341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/climatewavers/waverX-NLP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bd2bb2-7554-4e6a-b682-00bcff0c1faf",
   "metadata": {},
   "source": [
    "\n",
    "## Navigate to the model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "054a473b-9273-435d-9630-9c76afcecc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/src/waverX-NLP\n"
     ]
    }
   ],
   "source": [
    "%cd waverX-NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fcd003-7e01-4916-96d2-dbecffc6f93e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install Requirements\n",
    "\n",
    "\n",
    "- torch\n",
    "- transformers\n",
    "- tqdm\n",
    "- scikit-learn\n",
    "- numpy\n",
    "- pandas\n",
    "- keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af615c7-b129-479d-b4a3-1b8cd3a40020",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef0bd9-d55a-4fdd-9132-58774e8ec6b6",
   "metadata": {},
   "source": [
    "## Data Generation and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a45a818-eb02-4bd1-9014-436b8ac843e4",
   "metadata": {},
   "source": [
    "Data used in building was generated manually, from chatGPT and the tweets data  provide by Kaggle during the Real or Not? NLP with Disaster Tweets competition.Run below command to take a look at Kaggle data we made use of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45c1a0-5bb2-4803-9db4-64b3ab71e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat dataset/tweets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f30a8cd-5ffe-4391-9b70-4f4fc12ba0be",
   "metadata": {},
   "source": [
    "### Building Model Dataset\n",
    "We clean, proccess our data and build our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85bee40b-f897-4ef7-b39e-4abf0f6a4a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to dataset/tweets.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\"\"\"\n",
    "Filter Kaggle data\n",
    "\"\"\"\n",
    "def filter_data(input_file, output_file):\n",
    "    # Read CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Drop rows where target column is 0\n",
    "    # target = 0 represent tweet not related to disaster\n",
    "    cleaned_df = df[df['target'] != 0]\n",
    "    # Write cleaned DataFrame back to a CSV file\n",
    "    cleaned_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Cleaned data saved to {output_file}\")\n",
    "\n",
    "# Filter data and store in same file\n",
    "filter_data(\"dataset/tweets.csv\", \"dataset/tweets.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef3913-5d70-464f-87e5-06f2d1e702ae",
   "metadata": {},
   "source": [
    "We process our Kaggle data to concatenate them to our data generated from chatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7bebc75-9a84-46a1-9ed4-8db866e4755f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generated\n",
      "Ensured all data are of type str\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\"\"\"\n",
    "Anaylse data, filter categories and append right data to dataset\n",
    "\"\"\"\n",
    "\n",
    "tweet = pd.read_csv(\"dataset/tweets.csv\")\n",
    "\n",
    "# Model labels\n",
    "labels = [\"Earthquake\", \"Drought\",\n",
    "          \"Damaged Infrastructure\", \"Human Damage\", \"Human\", \"Land Slide\", \"Non Damage Buildings and  Street\", \"Non Damage Wildlife Forest\",\n",
    "          \"Sea\", \"Urban Fire\", \"Wild Fire\", \"Water Disaster\"]\n",
    "\n",
    "\n",
    "for index, row in tweet.iterrows():\n",
    "    keyword = row[\"keyword\"]\n",
    "    keyword = keyword.capitalize()\n",
    "    if keyword == \"Aftershock\":\n",
    "        keyword = \"Earthquake\"\n",
    "    elif keyword == \"Bridge collapse\":\n",
    "        keyword = \"Damaged Infrastructure\"\n",
    "    elif keyword == \"Buildings burning\" or keyword == \"Buildings on fire\":\n",
    "        keyword = \"Urban Fire\"\n",
    "    elif keyword == \"Burning\" or keyword == \"Burned\" or keyword == \"Bush fires\":\n",
    "        keyword = \"Wild Fire\"\n",
    "    elif keyword == \"Catastrophic\":\n",
    "        if \"fire\" in row[\"text\"]:\n",
    "            keyword = \"Wild Fire\"\n",
    "        elif \"earthquake\" in row[\"text\"]:\n",
    "            keyword = \"Earthquake\"\n",
    "    elif \"flood\" in keyword:\n",
    "        keyword = \"Water Disaster\"\n",
    "    elif \"wild\" in keyword:\n",
    "        keyword = \"Wild Fire\"\n",
    "    if keyword in labels:\n",
    "        text = str(row[\"text\"]).replace(\" \", \"_\")\n",
    "        label = keyword\n",
    "        dataset = pd.DataFrame([[text, label]])\n",
    "        dataset.to_csv(\"dataset/\" + \"disaster_text.csv\",\n",
    "                   mode='a', header=False, index=False)\n",
    "\n",
    "print(\"Dataset generated\")\n",
    "\n",
    "\n",
    "def add_data_type(input_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    #Make sure all data are of same type of string\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    df['label'] = df['label'].astype(str)\n",
    "    print(\"Ensured all data are of type str\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    add_data_type(\"dataset/disaster_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1b156-dafd-4439-94dc-0c1fb36cfcdf",
   "metadata": {},
   "source": [
    "### Split Dataset\n",
    "Split our dataset into train, validate, and test sets in the following percentage - 80%, 10%, 10% respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f76c1-89bb-4e92-b0dc-1ca668423c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Dataset split ----\n",
      "Total train dataset == 320\n",
      "Total test dataset == 41\n",
      "Total validate dataset == 40\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "df = pd.read_csv(\"dataset/disaster_text.csv\")\n",
    "\n",
    "# Split the dataset into train, validate, and test sets (80%, 10%, 10%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "validate_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save the datasets to CSV files\n",
    "train_df.to_csv('dataset/train_disaster_dataset.csv', index=False)\n",
    "validate_df.to_csv('dataset/val_disaster_dataset.csv', index=False)\n",
    "test_df.to_csv('dataset/test_disaster_dataset.csv', index=False)\n",
    "\n",
    "print(\"-----Dataset split ----\")\n",
    "print(f\"Total train dataset == {len(train_df)}\")\n",
    "print(f\"Total test dataset == {len(test_df)}\")\n",
    "print(f\"Total validate dataset == {len(validate_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6c4bf8-c25a-4b18-b088-383635460b76",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2f3ed-5238-4b1e-a540-c41115dfa3bb",
   "metadata": {},
   "source": [
    "### Process Dataset\n",
    "Load dataset to process, tokenize and have ready to use with our dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7725f92b-2194-4314-ab43-57cdd3df988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing texts\n",
      "tokenizing train texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:00<00:00, 1764.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert tokens to ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:00<00:00, 38741.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad our text tokens for each sequence\n",
      "tokenizing val texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 1664.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert tokens to ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 27494.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad our text tokens for each sequence\n",
      "tokenizing test texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 1689.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert tokens to ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 35471.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad our text tokens for each sequence\n",
      "creating train attention masks for texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:00<00:00, 4598.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating val attention masks for texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 4219.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating test attention masks for texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 4136.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting all variables to tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "## setting the threshold of logger to INFO\n",
    "logging.basicConfig(filename='data_loader.log', level=logging.INFO)\n",
    "\n",
    "## creating an object\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "\n",
    "class DisastersData:\n",
    "    def __init__(self, data_path, max_sequence_length=512):\n",
    "        \"\"\"\n",
    "        Load dataset and bert tokenizer\n",
    "        \"\"\"\n",
    "        ## load data into memory\n",
    "        self.train_df = pd.read_csv(data_path['train'])\n",
    "        self.val_df = pd.read_csv(data_path['val'])\n",
    "        self.test_df = pd.read_csv(data_path['test'])\n",
    "        ## set max sequence length for model\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        ## get bert tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-mini', do_lower_case=True)\n",
    "        self.tokenizer.save_pretrained(\"model/tokenizer\")\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(self.train_df['label'].values)\n",
    "\n",
    "    def train_val_test_split(self):\n",
    "        \"\"\"\n",
    "        Separate out labels and texts\n",
    "        \"\"\"\n",
    "        train_texts = self.train_df['text'].values\n",
    "        train_labels = self.label_encoder.transform(self.train_df['label'].values)\n",
    "        val_texts = self.val_df['text'].values\n",
    "        val_labels =  self.label_encoder.transform(self.val_df['label'].values)\n",
    "        test_texts = self.test_df['text'].values\n",
    "        test_labels =  self.label_encoder.transform(self.test_df['label'].values)\n",
    "\n",
    "        return train_texts, val_texts, test_texts, train_labels, val_labels, test_labels\n",
    "\n",
    "    def preprocess(self, texts):\n",
    "        \"\"\"\n",
    "        Add bert token (CLS and SEP) tokens to each sequence pre-tokenization\n",
    "        \"\"\"\n",
    "        ## separate labels and texts before preprocessing\n",
    "        # Adding CLS and SEP tokens at the beginning and end of each sequence for BERT\n",
    "        texts_processed = [\"[CLS] \" + str(sequence) + \" [SEP]\" for sequence in texts]\n",
    "        return texts_processed\n",
    "\n",
    "    def tokenize(self, texts):\n",
    "        \"\"\"\n",
    "        Use bert tokenizer to tokenize each sequence and post-process\n",
    "        by padding or truncating to a fixed length\n",
    "        \"\"\"\n",
    "        ## tokenize sequence\n",
    "        tokenized_texts = [self.tokenizer.tokenize(text) for text in tqdm(texts)]\n",
    "\n",
    "        ## convert tokens to ids\n",
    "        print('convert tokens to ids')\n",
    "        text_ids = [self.tokenizer.convert_tokens_to_ids(x) for x in tqdm(tokenized_texts)]\n",
    "\n",
    "        ## pad our text tokens for each sequence\n",
    "        print('pad our text tokens for each sequence')\n",
    "        text_ids_post_processed = pad_sequences(text_ids,\n",
    "                                       maxlen=self.max_sequence_length,\n",
    "                                       dtype=\"long\",\n",
    "                                       truncating=\"post\",\n",
    "                                       padding=\"post\")\n",
    "        return text_ids_post_processed\n",
    "\n",
    "    def create_attention_mask(self, text_ids):\n",
    "        \"\"\"\n",
    "        Add attention mask for padding tokens\n",
    "        \"\"\"\n",
    "        attention_masks = []\n",
    "        # create a mask of 1s for each token followed by 0s for padding\n",
    "        for seq in tqdm(text_ids):\n",
    "            seq_mask = [float(i>0) for i in seq]\n",
    "            attention_masks.append(seq_mask)\n",
    "        return attention_masks\n",
    "\n",
    "    def process_texts(self):\n",
    "        \"\"\"\n",
    "        Apply preprocessing and tokenization pipeline of texts\n",
    "        \"\"\"\n",
    "        ## perform the split\n",
    "        train_texts, val_texts, test_texts, train_labels, val_labels, test_labels = self.train_val_test_split()\n",
    "\n",
    "        print('preprocessing texts')\n",
    "        ## preprocess train, val, test texts\n",
    "        train_texts_processed = self.preprocess(train_texts)\n",
    "        val_texts_processed = self.preprocess(val_texts)\n",
    "        test_texts_processed = self.preprocess(test_texts)\n",
    "\n",
    "        del train_texts\n",
    "        del val_texts\n",
    "        del test_texts\n",
    "\n",
    "        ## preprocess train, val, test texts\n",
    "        print('tokenizing train texts')\n",
    "        train_ids = self.tokenize(train_texts_processed)\n",
    "        print('tokenizing val texts')\n",
    "        val_ids = self.tokenize(val_texts_processed)\n",
    "        print('tokenizing test texts')\n",
    "        test_ids = self.tokenize(test_texts_processed)\n",
    "\n",
    "        del train_texts_processed\n",
    "        del val_texts_processed\n",
    "        del test_texts_processed\n",
    "\n",
    "        del self.train_df\n",
    "        del self.val_df\n",
    "        del self.test_df\n",
    "\n",
    "        ## create masks for train, val, test texts\n",
    "        print('creating train attention masks for texts')\n",
    "        train_masks = self.create_attention_mask(train_ids)\n",
    "        print('creating val attention masks for texts')\n",
    "        val_masks = self.create_attention_mask(val_ids)\n",
    "        print('creating test attention masks for texts')\n",
    "        test_masks = self.create_attention_mask(test_ids)\n",
    "        return (\n",
    "                train_ids,\n",
    "                val_ids,\n",
    "                test_ids,\n",
    "                train_masks,\n",
    "                val_masks,\n",
    "                test_masks,\n",
    "                train_labels,\n",
    "                val_labels,\n",
    "                test_labels\n",
    "                )\n",
    "\n",
    "\n",
    "    def text_to_tensors(self):\n",
    "        \"\"\"\n",
    "        Converting all the data into torch tensors\n",
    "        \"\"\"\n",
    "        train_ids,  val_ids, test_ids, \\\n",
    "        train_masks, val_masks, test_masks, \\\n",
    "        train_labels, val_labels, test_labels = self.process_texts()\n",
    "\n",
    "        print('converting all variables to tensors')\n",
    "        ## convert inputs, masks and labels to torch tensors\n",
    "        self.train_inputs = torch.tensor(train_ids)\n",
    "        self.train_labels = torch.tensor(train_labels)\n",
    "        self.train_masks = torch.tensor(train_masks)\n",
    "\n",
    "        self.validation_inputs = torch.tensor(val_ids)\n",
    "        self.validation_labels = torch.tensor(val_labels)\n",
    "        self.validation_masks = torch.tensor(val_masks)\n",
    "\n",
    "        self.test_inputs = torch.tensor(test_ids)\n",
    "        self.test_labels = torch.tensor(test_labels)\n",
    "        self.test_masks = torch.tensor(test_masks)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_path = {\n",
    "        'train': 'dataset/train_disaster_dataset.csv',\n",
    "        'val': 'dataset/val_disaster_dataset.csv',\n",
    "        'test': 'dataset/test_disaster_dataset.csv'\n",
    "    }\n",
    "    DisastersData(data_path).text_to_tensors()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fcda37-928a-4462-bee3-86fe6bb892b8",
   "metadata": {},
   "source": [
    "### Building Model Dataloader\n",
    "Let build our model dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "611be716-57d9-4ec0-a4da-874ebf62240f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing texts\n",
      "tokenizing train texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:00<00:00, 1612.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert tokens to ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 320/320 [00:00<00:00, 35148.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad our text tokens for each sequence\n",
      "tokenizing val texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 1713.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert tokens to ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 36994.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad our text tokens for each sequence\n",
      "tokenizing test texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 1833.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert tokens to ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 35530.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad our text tokens for each sequence\n",
      "creating train attention masks for texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:00<00:00, 4437.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating val attention masks for texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 4721.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating test attention masks for texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 4365.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting all variables to tensors\n",
      "creating dataloaders\n",
      "finished creating dataloaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from process_dataset import DisastersData\n",
    "\n",
    "\n",
    "class DisastersDataLoader:\n",
    "\n",
    "    def __init__(self, data_file, batch_size=8):\n",
    "        self.data = DisastersData(data_file)\n",
    "        self.batch_size = batch_size\n",
    "        self.create_loaders()\n",
    "\n",
    "    def create_loaders(self):\n",
    "        \"\"\"\n",
    "        Create Torch dataloaders for data splits\n",
    "        \"\"\"\n",
    "        self.data.text_to_tensors()\n",
    "        print('creating dataloaders')\n",
    "        train_data = TensorDataset(self.data.train_inputs,\n",
    "                                    self.data.train_masks,\n",
    "                                    self.data.train_labels)\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        self.train_dataloader = DataLoader(train_data,\n",
    "                                            sampler=train_sampler,\n",
    "                                            batch_size=self.batch_size)\n",
    "\n",
    "        validation_data = TensorDataset(self.data.validation_inputs,\n",
    "                                        self.data.validation_masks,\n",
    "                                        self.data.validation_labels)\n",
    "        validation_sampler = SequentialSampler(validation_data)\n",
    "        self.validation_dataloader = DataLoader(validation_data,\n",
    "                                                sampler=validation_sampler,\n",
    "                                                batch_size=self.batch_size)\n",
    "\n",
    "        test_data = TensorDataset(self.data.test_inputs,\n",
    "                                        self.data.test_masks,\n",
    "                                        self.data.test_labels)\n",
    "        test_sampler = SequentialSampler(test_data)\n",
    "        self.test_dataloader = DataLoader(test_data,\n",
    "                                                sampler=test_sampler,\n",
    "                                                batch_size=self.batch_size)\n",
    "        print('finished creating dataloaders')\n",
    "\n",
    "if __name__=='__main__':\n",
    "    data_path = {\n",
    "        'train': 'dataset/train_disaster_dataset.csv',\n",
    "        'val': 'dataset/val_disaster_dataset.csv',\n",
    "        'test': 'dataset/test_disaster_dataset.csv'\n",
    "    }\n",
    "    loader = DisastersDataLoader(data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa797e0a-3900-45fb-ae39-1bcc200004cd",
   "metadata": {},
   "source": [
    "### Load Bert Model\n",
    "BERT model is initialized and fine-tuned using the Hugging Face Transformers library. BERT model is loaded using the from_pretrained() method. \"prajjwal1/bert-mini\" specifies the pre-trained model available in the Hugging Face model hub. The num_labels parameter is set based on the number of classes in your specific classification task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dd6e186-6401-4db9-b841-085bc41f640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "class BERTClassifier:\n",
    "    def __init__(self, num_labels=13):\n",
    "        self.configuration = BertConfig()\n",
    "\n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        Initialize pretrained bert model from huggingface model hub\n",
    "        \"\"\"\n",
    "        # initializing a model from the bert-base-uncased style configuration\n",
    "        model = BertModel(self.configuration)\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            \"prajjwal1/bert-mini\", num_labels=13\n",
    "        )\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b9c4d1-e5ad-4b5a-b969-121385dacc67",
   "metadata": {},
   "source": [
    "### Bert Model Configuration\n",
    "The bert model configuration is specified here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4234cb-87fc-4e75-a2d3-ef602b5d7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "class BertOptimConfig:\n",
    "    def __init__(self, model, train_dataloader, epochs=2):\n",
    "        # Don't apply weight decay to any parameters whose names include these tokens.\n",
    "        # (Here, the BERT doesn't have `gamma` or `beta` parameters, only `bias` terms)\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # Separate the `weight` parameters from the `bias` parameters.\n",
    "        # - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01.\n",
    "        # - For the `bias` parameters, the 'weight_decay_rate' is 0.0.\n",
    "        optimizer_grouped_parameters = [\n",
    "            # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay_rate': 0.1},\n",
    "\n",
    "            # Filter for parameters which *do* include those.\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "            'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "        # Note - `optimizer_grouped_parameters` only includes the parameter values, not\n",
    "        # the names.\n",
    "\n",
    "        # Number of training epochs (authors recommend between 2 and 4)\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                        )\n",
    "        # Total number of training steps is number of batches * number of epochs.\n",
    "        # `train_dataloader` contains batched data so `len(train_dataloader)` gives\n",
    "        # us the number of batches.\n",
    "        total_steps = len(train_dataloader) * self.epochs\n",
    "\n",
    "        ## create the learning rate scheduler.\n",
    "        self.scheduler = get_linear_schedule_with_warmup(self.optimizer,\n",
    "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                    num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c24965-6d9f-4d07-845a-d8b2181390f3",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Training model function that trains our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b094adb6-abf0-4123-8646-7d2fcc4e3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.accuracy import flat_accuracy\n",
    "from tqdm import trange\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, optimizer, scheduler, train_dataloader, validation_dataloader, epochs, device\n",
    "):\n",
    "    t = []\n",
    "\n",
    "    # Store our loss and accuracy for plotting\n",
    "    train_loss_set = []\n",
    "    \n",
    "    print(\"Training model\")\n",
    "    # trange is a tqdm wrapper around the normal python range\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "        ## set our model to training mode\n",
    "        model.train()\n",
    "\n",
    "        ## tracking variables\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        # train the model for one epoch\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            ## move batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            ## unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            ## reset the gradients\n",
    "            optimizer.zero_grad()\n",
    "            ## forward pass\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            loss, logits = outputs[:2]\n",
    "            train_loss_set.append(loss.item())\n",
    "            ## backward pass\n",
    "            loss.backward()\n",
    "            ## update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            ## update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "            ## update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "        print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))\n",
    "\n",
    "        # Put model in evaluation mode to evaluate loss on the validation set\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            # add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # avoiding model's computation and storage of gradients -> saving memory and speeding up validation\n",
    "            with torch.no_grad():\n",
    "                # forward pass, calculate logit predictions\n",
    "                logits = model(\n",
    "                    b_input_ids, token_type_ids=None, attention_mask=b_input_mask\n",
    "                )\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits[0].detach().cpu().numpy()\n",
    "            label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "            tmp_eval_accuracy = accuracy_score(label_ids, logits)\n",
    "\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "        model.save_pretrained(\"./model\")\n",
    "        print(\"Validation Accuracy: {}\".format(eval_accuracy / nb_eval_steps))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f792224-3bc0-4092-a9b5-70dcd554ec9d",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "We create our model evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19de86-9e66-4ee1-b345-3a6e99c21cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.accuracy import flat_accuracy\n",
    "\n",
    "\n",
    "def eval_model(model, test_dataloader, device):\n",
    "    ## tracking variables\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    ## evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        ## add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        ## unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        ## avoiding model's computation and storage of gradients -> saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # forward pass, calculate logit predictions\n",
    "            logits = model(\n",
    "                b_input_ids, token_type_ids=None, attention_mask=b_input_mask\n",
    "            )\n",
    "\n",
    "        ## move logits and labels to CPU\n",
    "        logits = logits[0].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    print(\"Test Accuracy: {}\".format(eval_accuracy / nb_eval_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c2f06-a90d-4e09-8454-41c73a7593e2",
   "metadata": {},
   "source": [
    "### Putting All Pieces Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f75b80e7-5913-4d33-ad9d-e5be63a5638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing texts\n",
      "tokenizing train texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:00<00:00, 1720.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert tokens to ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:00<00:00, 41667.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad our text tokens for each sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing val texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 1728.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert tokens to ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 35726.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad our text tokens for each sequence\n",
      "tokenizing test texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 1800.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert tokens to ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 35692.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad our text tokens for each sequence\n",
      "creating train attention masks for texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:00<00:00, 4363.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating val attention masks for texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 4311.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating test attention masks for texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 4357.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting all variables to tensors\n",
      "creating dataloaders\n",
      "finished creating dataloaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/app-root/miniconda3/envs/oneAPI-AIKit-DLPackage-with-PyTorch/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.4822049736976624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  14%|█▍        | 1/7 [00:59<05:59, 59.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.25\n",
      "Train loss: 2.304527533054352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  29%|██▊       | 2/7 [01:57<04:51, 58.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.375\n",
      "Train loss: 2.1314227849245073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  43%|████▎     | 3/7 [02:54<03:51, 57.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.85\n",
      "Train loss: 2.008513242006302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  57%|█████▋    | 4/7 [03:51<02:52, 57.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.925\n",
      "Train loss: 1.9165636211633683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  71%|███████▏  | 5/7 [04:45<01:52, 56.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.925\n",
      "Train loss: 1.8544430553913116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  86%|████████▌ | 6/7 [05:40<00:55, 55.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.95\n",
      "Train loss: 1.8229126304388046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 7/7 [06:37<00:00, 56.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7291666666666666\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bert import BERTClassifier\n",
    "from config import BertOptimConfig\n",
    "from train_model import train_model\n",
    "from evaluate import eval_model\n",
    "from data_loader import DisastersDataLoader\n",
    "\n",
    "\n",
    "epochs = 7\n",
    "num_labels = 13\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_path = {\n",
    "    \"train\": \"dataset/train_disaster_dataset.csv\",\n",
    "    \"val\": \"dataset/val_disaster_dataset.csv\",\n",
    "    \"test\": \"dataset/test_disaster_dataset.csv\",\n",
    "}\n",
    "data_loaders = DisastersDataLoader(data_path, batch_size=8)\n",
    "model = BERTClassifier(num_labels=num_labels).get_model()\n",
    "optim_config = BertOptimConfig(\n",
    "    model=model, train_dataloader=data_loaders.train_dataloader, epochs=epochs\n",
    ")\n",
    "    ## execute the training routine\n",
    "model = train_model(\n",
    "    model=model,\n",
    "    optimizer=optim_config.optimizer,\n",
    "    scheduler=optim_config.scheduler,\n",
    "    train_dataloader=data_loaders.train_dataloader,\n",
    "    validation_dataloader=data_loaders.validation_dataloader,\n",
    "    epochs=epochs,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "## test model performance on unseen test set\n",
    "eval_model(model=model, test_dataloader=data_loaders.test_dataloader, device=device)\n",
    "\n",
    "## Save model and tokenizer\n",
    "model.save_pretrained(\"model/waverx-nlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed2ed7-2e62-4d4a-bf52-272d9e3b40ef",
   "metadata": {},
   "source": [
    "## Run Inference with Intel Pytorch Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e04528d-23eb-4a28-be29-896e196d8f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting input\n",
      "Predicted Original Label: Earthquake\n",
      "Predicted Probabilities: {'Damaged Infrastructure': 7.9579620361328125, 'Drought': 11.536685943603516, 'Earthquake': 21.99901580810547, 'Human': 4.627230167388916, 'Human Damage': 6.254914283752441, 'Humanitarian Aid': 4.814874172210693, 'Land Slide': 4.5093889236450195, 'Non Damage Buildings and  Street': 6.152083873748779, 'Non Damage Wildlife Forest': 3.9017553329467773, 'Sea': 4.942859172821045, 'Urban Fire': 8.086578369140625, 'Water Disaster': 5.939942359924316, 'Wild Fire': 9.276710510253906}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from process_dataset import DisastersData\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "# Load the fine-tuned BERT model and tokenizer\n",
    "model_path = \"model/waverx-nlp\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"model/tokenizer\")\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "model = ipex.optimize(model)\n",
    "\n",
    "# Define your tokenized labels and mapping dictionary or list\n",
    "tokenized_labels =  [\"Earthquake\", \"Drought\",\n",
    "          \"Damaged Infrastructure\", \"Human Damage\", \"Human\", \"Land Slide\", \"Non Damage Buildings and  Street\", \"Non Damage Wildlife Forest\",\n",
    "          \"Sea\", \"Urban Fire\", \"Wild Fire\", \"Water Disaster\", \"Humanitarian Aid\"]\n",
    "\n",
    "tokenized_labels.sort()\n",
    "\n",
    "def predict(input_text):\n",
    "\n",
    "    # Tokenize input text\n",
    "    tokenized_input = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_input)\n",
    "\n",
    "    # Get predicted probabilities and labels\n",
    "    probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "    predicted_label_idx = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    token_to_label_mapping = {idx: label for idx, label in enumerate(tokenized_labels)}\n",
    "\n",
    "    # Map predicted label index to original label\n",
    "    predicted_original_label = token_to_label_mapping[predicted_label_idx]\n",
    "\n",
    "    # Convert tensor to numpy array\n",
    "    probabilities = probabilities.numpy()\n",
    "\n",
    "    # Convert probabilities to percentages\n",
    "    probabilities_percentage = probabilities * 100\n",
    "    \n",
    "    #Set probability threshold\n",
    "    threshold = 15\n",
    "    \n",
    "    # Find the maximum probability percentage\n",
    "    max_probability = max(probabilities_percentage.tolist()[0])\n",
    "    \n",
    "    if max_probability < threshold:\n",
    "    # Assign a specific label when the maximum probability is below the threshold\n",
    "        predicted_label = \"Prediction Failed\"\n",
    "    \n",
    "    # Create a dictionary from the list of probabilities and labels\n",
    "    result_dict = {key: value for key, value in zip(tokenized_labels, probabilities_percentage.tolist()[0])}\n",
    "\n",
    "    print(\"Predicted Label:\", predicted_original_label)\n",
    "    print(\"Prediction Probabilities:\",result_dict)\n",
    "    \n",
    "    return {\"prediction\": predicted_original_label,\n",
    "            \"probability\": result_dict }\n",
    "\n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Input text you want to classify\n",
    "    input_text = \" Urgent earthquake.\"\n",
    "    print(\"Predicting input\")\n",
    "    predict(input_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd60f9c-077f-4c5a-9a63-314d503c7f43",
   "metadata": {},
   "source": [
    "## Serve Model on Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126c7fb-6e78-4b84-bf14-5711235489e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\"\"\" Flask Application \"\"\"\n",
    "\n",
    "from os import environ\n",
    "from flask import Flask, jsonify, request\n",
    "from flask_cors import CORS\n",
    "from prediction import predict\n",
    "import json\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True\n",
    "cors = CORS(app, resources={r\"/api/v1/*\": {\"origins\": \"*\"}})\n",
    "\n",
    "@app.route(\"/api/v1/nlp/model/waverx\", methods=['POST'], strict_slashes=False)\n",
    "def model_inference():\n",
    "    data = request.data or '{}'\n",
    "    body = json.loads(data)\n",
    "    return jsonify(predict(body))\n",
    "\n",
    "@app.route(\"/api/v1/nlp/model/waverx/status\", strict_slashes=False)\n",
    "def model_status():\n",
    "    return jsonify({\"status\": \"OK\"})\n",
    "\n",
    "@app.errorhandler(404)\n",
    "def not_found(error):\n",
    "    \"\"\" 404 Error\n",
    "    ---\n",
    "    responses:\n",
    "      404:\n",
    "        description: a resource was not found\n",
    "    \"\"\"\n",
    "    return make_response(jsonify({'error': \"Not found\"}), 404)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Main Function \"\"\"\n",
    "    host = environ.get('WAVERX_HOST')\n",
    "    port = environ.get('WAVERX_PORT')\n",
    "    if not host:\n",
    "        host = '0.0.0.0'\n",
    "    if not port:\n",
    "        port = '5000'\n",
    "    app.run(host=host, port=port, threaded=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf8b1a1-bc14-4115-b71f-57793db3bb97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intel PyTorch & Quantization",
   "language": "python",
   "name": "oneapi-aikit-dlpackage-with-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
